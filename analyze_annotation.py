import os
import json
import numpy as np
from collections import Counter, defaultdict

base_dir = "data"

def main():
    create_dummy_predictions()
    read_data()

def create_dummy_predictions():
    for fn in os.listdir(base_dir):
        if fn.endswith("v0.jsonl"):
            prediction_dict = {}
            with open(os.path.join(base_dir, fn), "r") as f:
                for line in f:
                    dp = json.loads(line)
                    promptId = dp["promptId"]
                    response = dp["response"]

                    if response is not None:

                        fact_data = response["fact_data"]
                        predictions = []

                        '''
                        predictions is a list of dicts with two keys
                        - `sent_prediction`: prediction for the sentence
                        - `predictions`: prediction for each fact. `None` if the gold
                        sent_prediction is IR
                        '''

                        for sent_idx, sent_dp in enumerate(fact_data):

                            if sent_dp["sent_label"]!="IR":
                                assert sent_dp["facts"] is not None, sent_dp
                                curr_predictions = [
                                    np.random.choice(["S", "NS", "IR"])
                                    for _ in sent_dp["facts"]]
                            else:
                                curr_predictions = None

                            predictions.append({
                                "sent_prediction": np.random.choice(["S", "NS", "IR"]),
                                "predictions": curr_predictions
                            })

                        prediction_dict[promptId] = predictions


            with open(os.path.join(base_dir, fn.replace(".jsonl", "_predictions.json")), "w") as f:
                json.dump(prediction_dict, f)


def read_data():
    for fn in os.listdir(base_dir):
        if fn.endswith("v0.jsonl"):
            with open(os.path.join(base_dir, fn.replace(".jsonl", "_predictions.json")), "r") as f:
                sample_prediction_dict = json.load(f)

            '''metrics'''
            sent_accs = []
            sent_binary_accs = []
            accs = []
            binary_accs = []

            with open(os.path.join(base_dir, fn), "r") as f:
                for line in f:
                    dp = json.loads(line)

                    promptId = dp["promptId"] # _id
                    prompt = dp["prompt"] # input text fed into the base model
                    cat = dp["cat"] # category
                    response = dp["response"]

                    '''
                    response is None when the base model said `I don't know`
                    '''
                    if response is not None:

                        fact_data = response["fact_data"] # a list of setnences and atomic facts
                        coverage = response["coverage"] # label for coverage (good/okay/bad)
                        orig_text = response["orig_text"] # the original paragraph generated by the base model
                        final_text = response["final_text"] # the final paragraph edited by the worker

                        for sent_idx, sent_dp in enumerate(fact_data):
                            # for each sentence, we have the following data

                            orig_sentence = sent_dp["orig_sentence"] # original sentence (from the base model)
                            orig_facts = sent_dp["orig_facts"] # original fact (drafted by the model)

                            if sent_dp["sent_label"]!="IR":
                                # if the sentence is relevant, the worker revised the atomic facts
                                # (`facts`) and mark their correctness (`labels`)
                                # their lengths should be equal
                                assert len(sent_dp["facts"])==len(sent_dp["labels"])

                                # if any fact is not "S", the sent_label should be NS
                                # it is possible that, even if all facts are "S",
                                # the sent_label is NS
                                if not np.all([label=="S" for label in sent_dp["labels"]]):
                                    assert sent_dp["sent_label"] == "NS"
                                else:
                                    assert sent_dp["sent_label"] in ["S", "NS"]

                                if sent_dp["sent_label"]=="S":
                                    assert sent_dp["edited_sentence"].strip()==orig_sentence
                                else:
                                    assert sent_dp["edited_sentence"].strip()!=orig_sentence
                            else:
                                if not sent_dp["is_relevant"]:
                                    assert sent_dp["facts"] is None
                                    assert sent_dp["labels"] is None
                                    assert sent_dp["edited_sentence"] is None
                                    assert sent_dp["sent_label"] == "IR"


                        ##### this is a script for evaluating the predictions
                        predictions = sample_prediction_dict[promptId]
                        assert len(predictions)==len(fact_data)

                        curr_sent_accs = []
                        curr_sent_binary_accs = []
                        curr_accs = []
                        curr_binary_accs = []

                        for sent_idx, (sent_dp, prediction) in enumerate(zip(fact_data, predictions)):
                            sent_label = sent_dp["sent_label"]
                            sent_pred = prediction["sent_prediction"]
                            curr_sent_accs.append(sent_label==sent_pred)
                            curr_sent_binary_accs.append((sent_label=="S") == (sent_pred=="S"))

                            if sent_label=="IR":
                                assert prediction["predictions"] is None
                            else:
                                labels = sent_dp["labels"]
                                preds = prediction["predictions"]
                                assert len(labels)==len(preds)
                                for l, p in zip(labels, preds):
                                    curr_accs.append(l==p)
                                    curr_binary_accs.append((l=="S") == (p=="S"))

                        sent_accs.append(np.mean(curr_sent_accs))
                        sent_binary_accs.append(np.mean(curr_sent_binary_accs))

                        # it is possible curr_accs is empty if all sentences are IR
                        if len(curr_accs)>0:
                            accs.append(np.mean(curr_accs))
                            binary_accs.append(np.mean(curr_binary_accs))


            print ("Sent-level accuracy: %.1f%% (3-way) %.1f%% (2-way)" % (
                100*np.mean(sent_accs), 100*np.mean(sent_binary_accs)
            ))
            print ("Fact-level accuracy: %.1f%% (3-way) %.1f%% (2-way)" % (
                100*np.mean(accs), 100*np.mean(binary_accs)
            ))


def print_result(ss, nss, irs, ss1, nss1, irs1):
    print ("S: %.1f%%\tNS: %.1f%%\tIR: %.1f%%\tS: %.1f%%\tNS: %.1f%%\tIR: %.1f%%" % (
        np.mean(ss)*100, np.mean(nss)*100, np.mean(irs)*100,
        np.mean(ss1)*100, np.mean(nss1)*100, np.mean(irs1)*100,
    ))





if __name__=='__main__':
    main()